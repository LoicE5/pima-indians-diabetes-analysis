{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation du dataset & création de train et test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# https://datagy.io/pandas-shuffle-dataframe/#:~:text=One%20of%20the%20easiest%20ways,Dataframe%2C%20in%20a%20random%20order.\n",
    "df = pd.read_csv(\"../diabetes.csv\").sample(\n",
    "    frac = 1,\n",
    "    random_state=1\n",
    ").reset_index()\n",
    "\n",
    "y = df['Outcome']\n",
    "X = df.drop('Outcome', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=0)\n",
    "\n",
    "X_train:pd.DataFrame = X_train.to_numpy()\n",
    "X_test:pd.DataFrame = X_test.to_numpy()\n",
    "y_train:pd.DataFrame = y_train.to_numpy()\n",
    "y_test:pd.DataFrame = y_test.to_numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonction pour la classification en fonction de plusieurs algorithmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn_hierarchical_classification.classifier import HierarchicalClassifier\n",
    "\n",
    "from numpy import ndarray\n",
    "\n",
    "def calculate_classifier_accuracy(\n",
    "    X_train:ndarray,\n",
    "    X_test:ndarray,\n",
    "    y_train:ndarray,\n",
    "    y_test:ndarray,\n",
    "    classifier:KNeighborsClassifier|DecisionTreeClassifier|GaussianNB|Perceptron|MLPClassifier|KMeans|HierarchicalClassifier\n",
    "):\n",
    "    classifier.fit(X_train,y_train)\n",
    "\n",
    "    predicted = classifier.predict(X_test)\n",
    "\n",
    "    accuracy = [True if predicted[i] == y_test[i] else False for i in range(len(predicted))]\n",
    "    accuracy_stats = {\n",
    "        \"right\": len([i for i in accuracy if i]),\n",
    "        \"wrong\": len([i for i in accuracy if not i])\n",
    "    }\n",
    "    accuracy_stats[\"percentage\"] = round((accuracy_stats[\"right\"]/len(accuracy))*100,2)\n",
    "\n",
    "    return accuracy_stats\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K plus proches voisins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right': 102, 'wrong': 52, 'percentage': 66.23}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "calculate_classifier_accuracy(X_train, X_test, y_train, y_test, KNeighborsClassifier(n_neighbors=5,  metric='euclidean'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbre de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right': 113, 'wrong': 41, 'percentage': 73.38}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "calculate_classifier_accuracy(X_train, X_test, y_train, y_test, DecisionTreeClassifier())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification naïve bayésienne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right': 111, 'wrong': 43, 'percentage': 72.08}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "calculate_classifier_accuracy(X_train, X_test, y_train, y_test, GaussianNB())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right': 54, 'wrong': 100, 'percentage': 35.06}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "calculate_classifier_accuracy(X_train, X_test, y_train, y_test, Perceptron())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline\n",
    "\n",
    "Pour cet algorithme, n'ayant pas trouvé d'équivalent sur sklearn, nous utilisons le code de M. Ajitesh Kumar, trouvable sur le site VitalFlux.com, à l'adresse : https://vitalflux.com/adaline-explained-with-python-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_methods.py:48: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'right': 101, 'wrong': 53, 'percentage': 65.58}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomAdaline:\n",
    "    # Source : https://vitalflux.com/adaline-explained-with-python-example/\n",
    "     \n",
    "    def __init__(self, n_iterations=100, random_state=1, learning_rate=0.01):\n",
    "        self.n_iterations = n_iterations\n",
    "        self.random_state = random_state\n",
    "        self.learning_rate = learning_rate\n",
    " \n",
    "    '''\n",
    "    Batch Gradient Descent\n",
    "     \n",
    "    1. Weights are updated considering all training examples.\n",
    "    2. Learning of weights can continue for multiple iterations\n",
    "    3. Learning rate needs to be defined\n",
    "    '''\n",
    "    def fit(self, X, y):\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.coef_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n",
    "        for _ in range(self.n_iterations):\n",
    "              activation_function_output = self.activation_function(self.net_input(X))\n",
    "              errors = y - activation_function_output\n",
    "              self.coef_[1:] = self.coef_[1:] + self.learning_rate*X.T.dot(errors)\n",
    "              self.coef_[0] = self.coef_[0] + self.learning_rate*errors.sum()\n",
    "     \n",
    "    '''\n",
    "    Net Input is sum of weighted input signals\n",
    "    '''\n",
    "    def net_input(self, X):\n",
    "            weighted_sum = np.dot(X, self.coef_[1:]) + self.coef_[0]\n",
    "            return weighted_sum\n",
    "     \n",
    "    '''\n",
    "    Activation function is fed the net input. As the activation function is\n",
    "    an identity function, the output from activation function is same as the\n",
    "    input to the function.\n",
    "    '''\n",
    "    def activation_function(self, X):\n",
    "            return X\n",
    "     \n",
    "    '''\n",
    "    Prediction is made on the basis of output of activation function\n",
    "    '''\n",
    "    def predict(self, X):\n",
    "        return np.where(self.activation_function(self.net_input(X)) >= 0.0, 1, 0)\n",
    "     \n",
    "    '''\n",
    "    Model score is calculated based on comparison of\n",
    "    expected value and predicted value\n",
    "    '''\n",
    "    def score(self, X, y):\n",
    "        misclassified_data_count = 0\n",
    "        for xi, target in zip(X, y):\n",
    "            output = self.predict(xi)\n",
    "            if(target != output):\n",
    "                misclassified_data_count += 1\n",
    "        total_data_count = len(X)\n",
    "        self.score_ = (total_data_count - misclassified_data_count)/total_data_count\n",
    "        return self.score_\n",
    "\n",
    "calculate_classifier_accuracy(X_train, X_test, y_train, y_test, CustomAdaline())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseau de neurones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right': 101, 'wrong': 53, 'percentage': 65.58}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "calculate_classifier_accuracy(X_train, X_test, y_train, y_test, MLPClassifier(solver='lbfgs', alpha=1e-5 ,hidden_layer_sizes=(5, 2), random_state=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right': 101, 'wrong': 53, 'percentage': 65.58}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "calculate_classifier_accuracy(X_train, X_test, y_train, y_test, KMeans(n_clusters=1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification hiérarchique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right': 117, 'wrong': 37, 'percentage': 75.97}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_hierarchical_classification.classifier import HierarchicalClassifier\n",
    "\n",
    "calculate_classifier_accuracy(X_train, X_test, y_train, y_test, HierarchicalClassifier())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'right': 0, 'wrong': 154, 'percentage': 0.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "class DBSCAN_with_predict(DBSCAN):\n",
    "\n",
    "    def predict(self, X_new, metric=sp.spatial.distance.cosine):\n",
    "\n",
    "        # Result is noise by default\n",
    "        y_new = np.ones(shape=len(X_new), dtype=int)*-1\n",
    "\n",
    "        # Iterate all input samples for a label\n",
    "        for j, x_new in enumerate(X_new):\n",
    "            # Find a core sample closer than EPS\n",
    "            for i, x_core in enumerate(self.components_):\n",
    "                if metric(x_new, x_core) < self.eps:\n",
    "                    # Assign label of x_core to x_new\n",
    "                    y_new[j] = self.labels_[self.core_sample_indices_[i]]\n",
    "                    print(j,x_new,i,x_core,y_new)\n",
    "                    break\n",
    "\n",
    "        return y_new\n",
    "\n",
    "\n",
    "calculate_classifier_accuracy(X_train, X_test, y_train, y_test, DBSCAN_with_predict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
